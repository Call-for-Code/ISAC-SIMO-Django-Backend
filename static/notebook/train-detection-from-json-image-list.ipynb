{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISAC-SIMO-sample-model-generation-using-json-image-list-generated-from-isac-simo-image-share-request-page.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5EKJhqr1gz"
      },
      "source": [
        "### Image Object Model (.h5) Generation for Wall (Sample Example)\n",
        "## Uses **.json** image list file downloaded from image share/request page in ISAC-SIMO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79UE8NLVvl7g"
      },
      "source": [
        "#### Save the downloaded **.json** file in same directory as the Notebook (Or change the path below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQyc83kr1g7"
      },
      "source": [
        "import json\n",
        "\n",
        "OBJECT_TYPE = \"wall\" # What your the Object Type (and the JSON file name)\n",
        "# Replace json path as required\n",
        "f = open(OBJECT_TYPE+'.json', \"r\")\n",
        "walls = json.loads(f.read())\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNDUCpkfr1g0"
      },
      "source": [
        "## Install Prerequisites\n",
        "\n",
        "We use Keras/Tensorflow to build the classification model, and visualize the process with matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xjPdHZor1g1"
      },
      "source": [
        "!pip install tensorflow==2.5.0 ibm-cos-sdk==2.6 h5py==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6H0k7V3r1g4"
      },
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import uuid\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "import ibm_boto3\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XnKLRuRr1g6"
      },
      "source": [
        "## Download and Save images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfBjP61Mr1hB"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('data/'+OBJECT_TYPE, exist_ok=True)\n",
        "\n",
        "for wall in walls:\n",
        "  urllib.request.urlretrieve(wall.get(\"url\"), \"data/\"+OBJECT_TYPE+\"/\"+os.path.split(wall.get(\"url\"))[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1HMNpZYr1hD",
        "scrolled": true
      },
      "source": [
        "print(OBJECT_TYPE)\n",
        "!ls 'data/'$OBJECT_TYPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qpYZMNer1hG"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "We start with a [MobileNetV2](https://arxiv.org/abs/1801.04381) architecture as the backbone [pretrained feature extractor](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet). We then add a couple of dense layers and a softmax layer to perfom the classification. We freeze the MobileNetV2 backbone with weights trained on ImageNet dataset and only train the dense layers and softmax layer that we have added.\n",
        "\n",
        "Configuration needs to change depending on the image size and aspect ratio. Or you might use other backbone or libraries for training and exporting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN_FY40Dr1hG"
      },
      "source": [
        "base_model=tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3), alpha=1.0) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x=base_model.output\n",
        "x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x=tf.keras.layers.Dense(512,activation='relu')(x) #dense layer 1\n",
        "x=tf.keras.layers.Dense(256,activation='relu')(x) #dense layer 2\n",
        "preds=tf.keras.layers.Dense(3,activation='softmax')(x) #final layer with softmax activation\n",
        "\n",
        "model=tf.keras.Model(inputs=base_model.input,outputs=preds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApSMk3W2r1hJ"
      },
      "source": [
        "#Freeze layers from MobileNetV2 backbone (not to be trained)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHEvYJARr1hM"
      },
      "source": [
        "#Prepare the training dataset as a data generator object\n",
        "train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('data',\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=10,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv_R_SCFr1hO"
      },
      "source": [
        "#### Using Adam, categorical_crossentropy and accuracy as optimization method, loss function and metrics, respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93AFFFdtr1hO"
      },
      "source": [
        "# Build the model\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjj9cjU3r1hR"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exRobg6cr1hS",
        "scrolled": false
      },
      "source": [
        "from tensorflow.random import set_seed\n",
        "set_seed(3)\n",
        "step_size_train=1\n",
        "epochs=1\n",
        "log_file = model.fit(train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNRpVg-zr1hU"
      },
      "source": [
        "## Figure of Training Loss and Accuracy\n",
        "Plotting the training result as below (if required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C6J2Gubr1hV"
      },
      "source": [
        "# # Model accuracy and loss vs epoch\n",
        "# plt.plot(log_file.history['acc'], '-bo', label=\"train_accuracy\")\n",
        "# plt.plot(log_file.history['loss'], '-r*', label=\"train_loss\")\n",
        "# plt.title('Training Loss and Accuracy')\n",
        "# plt.ylabel('Loss/Accuracy')\n",
        "# plt.xlabel('Epoch #')\n",
        "# plt.legend(loc='center right')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3pu-2Ddr1hX"
      },
      "source": [
        "## Model Performance\n",
        "\n",
        "Here we perform inference on some sample data points to determine the performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD_GuzqAr1hX"
      },
      "source": [
        "# Mapping labels \n",
        "label_map = (train_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl-eHspBr1hZ"
      },
      "source": [
        "label_map\n",
        "# Returns Example: {'wall': 0}\n",
        "# Multiple object types (specially related) can be trained same model to make it re-useable."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQENLlakr1hc"
      },
      "source": [
        "# Creating a sample inference function\n",
        "def prediction(image_path, model):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "    preds = model.predict(x)\n",
        "    # print('Predictions', preds)\n",
        "    \n",
        "    #  Printing the prediction score and class\n",
        "    # for pred, value in label_map.items():    \n",
        "    #     if value == np.argmax(preds):\n",
        "    #         print('Predicted class is:', pred)\n",
        "    #         print('With a confidence score of: ', np.max(preds))\n",
        "\n",
        "    # Format the Output as required by ISAC-SIMO Offline Model (Object Detect)\n",
        "    # https://www.isac-simo.net/app/offline_model/readme.md#object-detect\n",
        "    if np.argmax(preds) == 0:\n",
        "      return [[np.max(preds)]] # Detected\n",
        "    else:\n",
        "      return [[0]] # Did not Detect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jJ3EsC0r1he"
      },
      "source": [
        "# Download some Sample Images (Example: Stirrup)\n",
        "wall_img = 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Fragment_muru_z_ceg%C5%82y.jpg/1280px-Fragment_muru_z_ceg%C5%82y.jpg'\n",
        "!wget {wall_img} -O wall.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lhwp01Fr1hs"
      },
      "source": [
        "# Opening test image\n",
        "image = Image.open(\"wall.jpg\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SH5afqlr1ht"
      },
      "source": [
        "prediction('wall.jpg', model)\n",
        "# Returns Example: [[0.69597286, 0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvwkd5W6CV7A"
      },
      "source": [
        "# Generate and Download the .h5 Model\n",
        "from google.colab import files\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/detect-model-for-'+OBJECT_TYPE+'.h5')\n",
        "files.download('saved_model/detect-model-for-'+OBJECT_TYPE+'.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}